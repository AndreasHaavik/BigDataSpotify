{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist_names</th>\n",
       "      <th>artists_num</th>\n",
       "      <th>artist_genre</th>\n",
       "      <th>collab</th>\n",
       "      <th>release_date</th>\n",
       "      <th>album_num_tracks</th>\n",
       "      <th>source</th>\n",
       "      <th>peak_rank</th>\n",
       "      <th>weeks_on_chart</th>\n",
       "      <th>streams</th>\n",
       "      <th>...</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jorge &amp; Mateus</td>\n",
       "      <td>1</td>\n",
       "      <td>sertanejo universitario, sertanejo universitar...</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-04-22</td>\n",
       "      <td>15</td>\n",
       "      <td>Som Livre</td>\n",
       "      <td>4</td>\n",
       "      <td>36.50000</td>\n",
       "      <td>37158272</td>\n",
       "      <td>...</td>\n",
       "      <td>0.620</td>\n",
       "      <td>5</td>\n",
       "      <td>-5.536</td>\n",
       "      <td>0.0509</td>\n",
       "      <td>0.30900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0750</td>\n",
       "      <td>0.962</td>\n",
       "      <td>117.399</td>\n",
       "      <td>164459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZAYN, PARTYNEXTDOOR</td>\n",
       "      <td>2</td>\n",
       "      <td>dance pop, r&amp;b, pop, pop rap, dance pop, r&amp;b, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-03-23</td>\n",
       "      <td>1</td>\n",
       "      <td>RCA Records Label</td>\n",
       "      <td>46</td>\n",
       "      <td>9.00000</td>\n",
       "      <td>215055522</td>\n",
       "      <td>...</td>\n",
       "      <td>0.627</td>\n",
       "      <td>7</td>\n",
       "      <td>-6.029</td>\n",
       "      <td>0.0639</td>\n",
       "      <td>0.13100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0852</td>\n",
       "      <td>0.524</td>\n",
       "      <td>120.963</td>\n",
       "      <td>188491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TAEYEON</td>\n",
       "      <td>1</td>\n",
       "      <td>k-pop, k-pop, k-pop</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-02-14</td>\n",
       "      <td>13</td>\n",
       "      <td>SM Entertainment</td>\n",
       "      <td>61</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>48580</td>\n",
       "      <td>...</td>\n",
       "      <td>0.683</td>\n",
       "      <td>8</td>\n",
       "      <td>-3.827</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.73700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.493</td>\n",
       "      <td>128.018</td>\n",
       "      <td>157987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alessia Cara</td>\n",
       "      <td>1</td>\n",
       "      <td>post-teen pop, dance pop</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-06-15</td>\n",
       "      <td>1</td>\n",
       "      <td>Def Jam Recordings</td>\n",
       "      <td>112</td>\n",
       "      <td>1.50000</td>\n",
       "      <td>9944865</td>\n",
       "      <td>...</td>\n",
       "      <td>0.755</td>\n",
       "      <td>1</td>\n",
       "      <td>-6.276</td>\n",
       "      <td>0.7330</td>\n",
       "      <td>0.08220</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.3900</td>\n",
       "      <td>0.437</td>\n",
       "      <td>191.153</td>\n",
       "      <td>193680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Killers</td>\n",
       "      <td>1</td>\n",
       "      <td>rock, dance rock, dance rock, rock, rock, perm...</td>\n",
       "      <td>0</td>\n",
       "      <td>2004</td>\n",
       "      <td>12</td>\n",
       "      <td>Island Records</td>\n",
       "      <td>44</td>\n",
       "      <td>129.30303</td>\n",
       "      <td>429376201</td>\n",
       "      <td>...</td>\n",
       "      <td>0.911</td>\n",
       "      <td>1</td>\n",
       "      <td>-5.230</td>\n",
       "      <td>0.0747</td>\n",
       "      <td>0.00121</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.236</td>\n",
       "      <td>148.033</td>\n",
       "      <td>222973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26252</th>\n",
       "      <td>Jonasu</td>\n",
       "      <td>1</td>\n",
       "      <td>house, house, house, pop dance, house, pop dan...</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-12-18</td>\n",
       "      <td>1</td>\n",
       "      <td>3Beat</td>\n",
       "      <td>4</td>\n",
       "      <td>27.00000</td>\n",
       "      <td>6885448</td>\n",
       "      <td>...</td>\n",
       "      <td>0.634</td>\n",
       "      <td>8</td>\n",
       "      <td>-4.960</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>0.05640</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2760</td>\n",
       "      <td>0.463</td>\n",
       "      <td>124.074</td>\n",
       "      <td>174194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26253</th>\n",
       "      <td>Montiego, Lvbel C5, Batuflex</td>\n",
       "      <td>3</td>\n",
       "      <td>persian hip hop, turkce drill, turkish trap, p...</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-02-04</td>\n",
       "      <td>1</td>\n",
       "      <td>Lais Records</td>\n",
       "      <td>7</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>421278</td>\n",
       "      <td>...</td>\n",
       "      <td>0.469</td>\n",
       "      <td>8</td>\n",
       "      <td>-7.666</td>\n",
       "      <td>0.1070</td>\n",
       "      <td>0.33300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0725</td>\n",
       "      <td>0.353</td>\n",
       "      <td>114.017</td>\n",
       "      <td>144737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26254</th>\n",
       "      <td>boy pablo</td>\n",
       "      <td>1</td>\n",
       "      <td>bedroom pop, bedroom pop, bedroom pop, bedroom...</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-10-05</td>\n",
       "      <td>7</td>\n",
       "      <td>U OK?</td>\n",
       "      <td>40</td>\n",
       "      <td>8.50000</td>\n",
       "      <td>11300755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.504</td>\n",
       "      <td>9</td>\n",
       "      <td>-10.003</td>\n",
       "      <td>0.0318</td>\n",
       "      <td>0.02200</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.3630</td>\n",
       "      <td>0.401</td>\n",
       "      <td>165.860</td>\n",
       "      <td>155714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26255</th>\n",
       "      <td>NAV, The Weeknd</td>\n",
       "      <td>2</td>\n",
       "      <td>melodic rap, canadian pop, trap, canadian cont...</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-02-24</td>\n",
       "      <td>11</td>\n",
       "      <td>XO Records</td>\n",
       "      <td>171</td>\n",
       "      <td>1.50000</td>\n",
       "      <td>13137028</td>\n",
       "      <td>...</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.103</td>\n",
       "      <td>0.3510</td>\n",
       "      <td>0.10100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0919</td>\n",
       "      <td>0.340</td>\n",
       "      <td>154.962</td>\n",
       "      <td>179773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26256</th>\n",
       "      <td>Pabllo Vittar</td>\n",
       "      <td>1</td>\n",
       "      <td>dance pop, funk carioca, pop, dance pop, funk ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-06-24</td>\n",
       "      <td>9</td>\n",
       "      <td>Mataderos</td>\n",
       "      <td>60</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>12089312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.864</td>\n",
       "      <td>8</td>\n",
       "      <td>-3.850</td>\n",
       "      <td>0.0400</td>\n",
       "      <td>0.08580</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1580</td>\n",
       "      <td>0.833</td>\n",
       "      <td>169.123</td>\n",
       "      <td>167503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26257 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       artist_names  artists_num  \\\n",
       "0                    Jorge & Mateus            1   \n",
       "1               ZAYN, PARTYNEXTDOOR            2   \n",
       "2                           TAEYEON            1   \n",
       "3                      Alessia Cara            1   \n",
       "4                       The Killers            1   \n",
       "...                             ...          ...   \n",
       "26252                        Jonasu            1   \n",
       "26253  Montiego, Lvbel C5, Batuflex            3   \n",
       "26254                     boy pablo            1   \n",
       "26255               NAV, The Weeknd            2   \n",
       "26256                 Pabllo Vittar            1   \n",
       "\n",
       "                                            artist_genre  collab release_date  \\\n",
       "0      sertanejo universitario, sertanejo universitar...       0   2021-04-22   \n",
       "1      dance pop, r&b, pop, pop rap, dance pop, r&b, ...       1   2017-03-23   \n",
       "2                                    k-pop, k-pop, k-pop       0   2022-02-14   \n",
       "3                               post-teen pop, dance pop       0   2018-06-15   \n",
       "4      rock, dance rock, dance rock, rock, rock, perm...       0         2004   \n",
       "...                                                  ...     ...          ...   \n",
       "26252  house, house, house, pop dance, house, pop dan...       0   2020-12-18   \n",
       "26253  persian hip hop, turkce drill, turkish trap, p...       1   2022-02-04   \n",
       "26254  bedroom pop, bedroom pop, bedroom pop, bedroom...       0   2018-10-05   \n",
       "26255  melodic rap, canadian pop, trap, canadian cont...       1   2017-02-24   \n",
       "26256  dance pop, funk carioca, pop, dance pop, funk ...       0   2021-06-24   \n",
       "\n",
       "       album_num_tracks              source  peak_rank  weeks_on_chart  \\\n",
       "0                    15           Som Livre          4        36.50000   \n",
       "1                     1   RCA Records Label         46         9.00000   \n",
       "2                    13    SM Entertainment         61         2.00000   \n",
       "3                     1  Def Jam Recordings        112         1.50000   \n",
       "4                    12      Island Records         44       129.30303   \n",
       "...                 ...                 ...        ...             ...   \n",
       "26252                 1               3Beat          4        27.00000   \n",
       "26253                 1        Lais Records          7        10.00000   \n",
       "26254                 7               U OK?         40         8.50000   \n",
       "26255                11          XO Records        171         1.50000   \n",
       "26256                 9           Mataderos         60         6.00000   \n",
       "\n",
       "         streams  ...  energy  key  loudness  speechiness  acousticness  \\\n",
       "0       37158272  ...   0.620    5    -5.536       0.0509       0.30900   \n",
       "1      215055522  ...   0.627    7    -6.029       0.0639       0.13100   \n",
       "2          48580  ...   0.683    8    -3.827       0.0523       0.73700   \n",
       "3        9944865  ...   0.755    1    -6.276       0.7330       0.08220   \n",
       "4      429376201  ...   0.911    1    -5.230       0.0747       0.00121   \n",
       "...          ...  ...     ...  ...       ...          ...           ...   \n",
       "26252    6885448  ...   0.634    8    -4.960       0.1630       0.05640   \n",
       "26253     421278  ...   0.469    8    -7.666       0.1070       0.33300   \n",
       "26254   11300755  ...   0.504    9   -10.003       0.0318       0.02200   \n",
       "26255   13137028  ...   0.715    0    -6.103       0.3510       0.10100   \n",
       "26256   12089312  ...   0.864    8    -3.850       0.0400       0.08580   \n",
       "\n",
       "       instrumentalness  liveness  valence    tempo  duration  \n",
       "0              0.000000    0.0750    0.962  117.399    164459  \n",
       "1              0.000000    0.0852    0.524  120.963    188491  \n",
       "2              0.000000    0.1140    0.493  128.018    157987  \n",
       "3              0.000000    0.3900    0.437  191.153    193680  \n",
       "4              0.000000    0.0995    0.236  148.033    222973  \n",
       "...                 ...       ...      ...      ...       ...  \n",
       "26252          0.000000    0.2760    0.463  124.074    174194  \n",
       "26253          0.000000    0.0725    0.353  114.017    144737  \n",
       "26254          0.000004    0.3630    0.401  165.860    155714  \n",
       "26255          0.000000    0.0919    0.340  154.962    179773  \n",
       "26256          0.000000    0.1580    0.833  169.123    167503  \n",
       "\n",
       "[26257 rows x 21 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculating a Dummy classfier - baseline for accuracy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Print column names to inspect\n",
    "cleaned_df = pd.read_excel('SpotifyDataCleaned.xlsx')\n",
    "\n",
    "cleaned_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((26257, 21), (25840, 21))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Load the dataset again\n",
    "file_path = './SpotifyDataCleaned.xlsx'\n",
    "spotify_data = pd.read_excel(file_path)\n",
    "\n",
    "# Considering 'streams' as the target variable for outlier detection\n",
    "target_variable = 'streams'\n",
    "\n",
    "# Detecting outliers in the target variable using Z-scores\n",
    "z_scores = np.abs(stats.zscore(spotify_data[target_variable]))\n",
    "\n",
    "# Define a threshold for identifying outliers (commonly a Z-score of 3 or more is considered as an outlier)\n",
    "threshold = 3\n",
    "outlier_indices = np.where(z_scores > threshold)\n",
    "\n",
    "# Removing the outliers from the dataset\n",
    "spotify_data_cleaned = spotify_data.drop(outlier_indices[0])\n",
    "\n",
    "# Shape of the data before and after outlier removal\n",
    "data_shape_before = spotify_data.shape\n",
    "data_shape_after = spotify_data_cleaned.shape\n",
    "\n",
    "data_shape_before, data_shape_after\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "THIS DESCRIBTION BELONGS TO THE CODE BELOW..! \n",
    "\n",
    "The Decision Tree Regression model has been trained and evaluated on your dataset. The Mean Squared Error (MSE) for the model's predictions on the test set is approximately 7.02 X 10^16\n",
    "\n",
    "This high MSE value suggests that the model's predictions are quite far from the actual values. This could be due to various factors such as:\n",
    "\n",
    "Model Complexity: Decision trees can easily overfit, especially if they grow very deep. It might be beneficial to tune parameters like the maximum depth of the tree.\n",
    "\n",
    "Feature Selection: The model may perform better with a different set of features. Consider including or excluding different features based on their relevance to the target variable.\n",
    "\n",
    "Data Scale: If some features have a much larger scale than others, it can affect the model's performance. Normalizing or standardizing the features might help.\n",
    "\n",
    "Outliers: The presence of outliers in the data can greatly affect the performance of a decision tree model. Investigating and handling outliers might improve the model's predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 5.953371432725916e+16\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Selecting relevant columns for the model\n",
    "# Assuming 'streams' is the target variable and excluding highly categorical columns for simplicity\n",
    "relevant_columns = ['artists_num', 'collab', 'album_num_tracks', 'peak_rank', 'weeks_on_chart', 'streams',\n",
    "                    'danceability', 'energy', 'key', 'loudness', 'speechiness', 'acousticness', 'instrumentalness',\n",
    "                    'liveness', 'valence', 'tempo', 'duration']\n",
    "data_for_model = cleaned_df[relevant_columns]\n",
    "\n",
    "# Encoding any remaining categorical variables\n",
    "label_encoders = {}\n",
    "for column in data_for_model.select_dtypes(include=['object']).columns:\n",
    "    label_encoders[column] = LabelEncoder()\n",
    "    data_for_model[column] = label_encoders[column].fit_transform(data_for_model[column])\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X = data_for_model.drop('streams', axis=1)\n",
    "y = data_for_model['streams']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initializing and training the Decision Tree Regressor\n",
    "dt_regressor = DecisionTreeRegressor(random_state=0)\n",
    "dt_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predicting and evaluating the model\n",
    "y_pred = dt_regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THIS DESCRIBTION BELONGS TO THE CODE BELOW..! \n",
    "Let's start by implementing some of these strategies. We can begin with hyperparameter tuning of the decision tree, along with feature importance analysis to guide feature selection. We can also ensure data scaling is appropriately handled.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 10},\n",
       " 3.7565107267722344e+16)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define a set of parameters for tuning\n",
    "param_grid = {\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "dt_regressor = DecisionTreeRegressor(random_state=0)\n",
    "grid_search = GridSearchCV(estimator=dt_regressor, param_grid=param_grid, cv=5, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "best_params = grid_search.best_params_\n",
    "best_mse = -grid_search.best_score_\n",
    "\n",
    "best_params, best_mse\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THIS DESCRIBTION BELONGS TO THE CODE BELOW..! \n",
    "To optimize the model within a practical time frame, we can try a few adjustments:\n",
    "\n",
    "Reduce the Search Space: Limit the range or number of values in param_grid. This will reduce the number of combinations the grid search needs to evaluate.\n",
    "\n",
    "Randomized Search: Instead of trying every combination like GridSearchCV, use RandomizedSearchCV which samples a fixed number of parameter combinations.\n",
    "\n",
    "Incremental Tuning: Manually adjust one or two parameters at a time rather than using grid search.\n",
    "\n",
    "Let's start with a reduced and more focused hyperparameter space for the Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 5},\n",
       " 3.19206259408831e+16)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define a reduced set of parameters for tuning\n",
    "param_grid_reduced = {\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object with reduced parameter grid\n",
    "grid_search_reduced = GridSearchCV(estimator=dt_regressor, param_grid=param_grid_reduced, cv=3, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Perform the grid search\n",
    "grid_search_reduced.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and MSE\n",
    "best_params_reduced = grid_search_reduced.best_params_\n",
    "best_mse_reduced = -grid_search_reduced.best_score_\n",
    "\n",
    "best_params_reduced, best_mse_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THIS DESCRIBTION BELONGS TO THE CODE BELOW..! \n",
    "We identify categorical columns and apply One-Hot Encoding to them.\n",
    "We use a pipeline to streamline the preprocessing and model training steps.\n",
    "The model is trained on the preprocessed data, and the performance is evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validated MSE: 3.6888329880300015\n",
      "Random Forest Cross-Validated MSE: 2.676027563627838\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create a copy of the dataset to avoid changing the original dataframe\n",
    "spotify_data_encoded = spotify_data_cleaned.copy()\n",
    "\n",
    "# Applying Label Encoding to all categorical columns\n",
    "label_encoders = {}\n",
    "for column in spotify_data_encoded.select_dtypes(include=['object']).columns:\n",
    "    label_encoders[column] = LabelEncoder()\n",
    "    spotify_data_encoded[column] = label_encoders[column].fit_transform(spotify_data_encoded[column])\n",
    "\n",
    "# Splitting the data\n",
    "X = spotify_data_encoded.drop(['streams', 'streams_log'], axis=1)\n",
    "y = spotify_data_encoded['streams_log']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Decision Tree and Random Forest models with Cross-Validation\n",
    "# (The rest of the code remains the same as in the previous example)\n",
    "\n",
    "\n",
    "# Adjust model complexity\n",
    "dt_regressor = DecisionTreeRegressor(max_depth=10, min_samples_split=50, min_samples_leaf=20, random_state=0)\n",
    "\n",
    "# Apply cross-validation\n",
    "cross_val_scores = cross_val_score(dt_regressor, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "mean_cv_mse = -cross_val_scores.mean()\n",
    "print(\"Cross-Validated MSE:\", mean_cv_mse)\n",
    "\n",
    "# Trying a different model - Random Forest\n",
    "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "rf_cross_val_scores = cross_val_score(rf_regressor, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "rf_mean_cv_mse = -rf_cross_val_scores.mean()\n",
    "print(\"Random Forest Cross-Validated MSE:\", rf_mean_cv_mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below removes outliers but it gives me an huge MSE - it is working though!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((26257, 21), (25840, 21))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Load the dataset again\n",
    "file_path = './SpotifyDataCleaned.xlsx'\n",
    "spotify_data = pd.read_excel(file_path)\n",
    "\n",
    "# Considering 'streams' as the target variable for outlier detection\n",
    "target_variable = 'streams'\n",
    "\n",
    "# Detecting outliers in the target variable using Z-scores\n",
    "z_scores = np.abs(stats.zscore(spotify_data[target_variable]))\n",
    "\n",
    "# Define a threshold for identifying outliers (commonly a Z-score of 3 or more is considered as an outlier)\n",
    "threshold = 3\n",
    "outlier_indices = np.where(z_scores > threshold)\n",
    "\n",
    "# Removing the outliers from the dataset\n",
    "spotify_data_cleaned = spotify_data.drop(outlier_indices[0])\n",
    "\n",
    "# Shape of the data before and after outlier removal\n",
    "data_shape_before = spotify_data.shape\n",
    "data_shape_after = spotify_data_cleaned.shape\n",
    "\n",
    "data_shape_before, data_shape_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['artist_names', 'artists_num', 'artist_genre', 'collab', 'release_date',\n",
      "       'album_num_tracks', 'source', 'peak_rank', 'weeks_on_chart', 'streams',\n",
      "       'danceability', 'energy', 'key', 'loudness', 'speechiness',\n",
      "       'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',\n",
      "       'duration'],\n",
      "      dtype='object')\n",
      "X_train shape: (18379, 20)\n",
      "X_test shape: (7878, 20)\n",
      "y_train shape: (18379,)\n",
      "y_test shape: (7878,)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "print(cleaned_df.columns)\n",
    "\n",
    "# Separate features (X) and labels (y)\n",
    "X = cleaned_df[['artist_names','artists_num','artist_genre','collab','release_date', 'album_num_tracks', 'source', 'peak_rank','weeks_on_chart', 'danceability', 'energy', 'key','loudness', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'duration']]\n",
    "y = cleaned_df['streams']  \n",
    "\n",
    "# Split the dataset into 70% training and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Display the shapes of the resulting sets\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error of the dummy regressor: 31474014057368556.00\n",
      "Root Mean Squared Error (RMSE): 177409171.29\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "# Create a dummy regressor (we create a regressor because we want to predict a numeric target variable)\n",
    "dummy_regressor = DummyRegressor(strategy='mean')  # You can also use 'median' or 'quantile'\n",
    "\n",
    "# Train the dummy regressor\n",
    "dummy_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = dummy_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the dummy regressor using mean squared error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"Mean Squared Error of the dummy regressor: {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Lil Baby'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n\u001b[1;32m      7\u001b[0m logr \u001b[38;5;241m=\u001b[39m LogisticRegression()\n\u001b[0;32m----> 8\u001b[0m logr\u001b[38;5;241m.\u001b[39mfit(X_train,y_train)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1207\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1204\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1205\u001b[0m     _dtype \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39mfloat64, np\u001b[39m.\u001b[39mfloat32]\n\u001b[0;32m-> 1207\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(\n\u001b[1;32m   1208\u001b[0m     X,\n\u001b[1;32m   1209\u001b[0m     y,\n\u001b[1;32m   1210\u001b[0m     accept_sparse\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcsr\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1211\u001b[0m     dtype\u001b[39m=\u001b[39m_dtype,\n\u001b[1;32m   1212\u001b[0m     order\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mC\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1213\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39msolver \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mliblinear\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msag\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msaga\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   1214\u001b[0m )\n\u001b[1;32m   1215\u001b[0m check_classification_targets(y)\n\u001b[1;32m   1216\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(y)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:621\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    619\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[1;32m    620\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 621\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[1;32m    622\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    624\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1147\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1142\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1143\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1144\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1145\u001b[0m     )\n\u001b[0;32m-> 1147\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m   1148\u001b[0m     X,\n\u001b[1;32m   1149\u001b[0m     accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[1;32m   1150\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39maccept_large_sparse,\n\u001b[1;32m   1151\u001b[0m     dtype\u001b[39m=\u001b[39mdtype,\n\u001b[1;32m   1152\u001b[0m     order\u001b[39m=\u001b[39morder,\n\u001b[1;32m   1153\u001b[0m     copy\u001b[39m=\u001b[39mcopy,\n\u001b[1;32m   1154\u001b[0m     force_all_finite\u001b[39m=\u001b[39mforce_all_finite,\n\u001b[1;32m   1155\u001b[0m     ensure_2d\u001b[39m=\u001b[39mensure_2d,\n\u001b[1;32m   1156\u001b[0m     allow_nd\u001b[39m=\u001b[39mallow_nd,\n\u001b[1;32m   1157\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39mensure_min_samples,\n\u001b[1;32m   1158\u001b[0m     ensure_min_features\u001b[39m=\u001b[39mensure_min_features,\n\u001b[1;32m   1159\u001b[0m     estimator\u001b[39m=\u001b[39mestimator,\n\u001b[1;32m   1160\u001b[0m     input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1161\u001b[0m )\n\u001b[1;32m   1163\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[1;32m   1165\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:917\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    915\u001b[0m         array \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(array, dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    916\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 917\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(array, order\u001b[39m=\u001b[39morder, dtype\u001b[39m=\u001b[39mdtype, xp\u001b[39m=\u001b[39mxp)\n\u001b[1;32m    918\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[1;32m    919\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    920\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[1;32m    921\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_array_api.py:380\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    378\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39marray(array, order\u001b[39m=\u001b[39morder, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m    379\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 380\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39masarray(array, order\u001b[39m=\u001b[39morder, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m    382\u001b[0m \u001b[39m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[39m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39masarray(array)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py:2070\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   2069\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype: npt\u001b[39m.\u001b[39mDTypeLike \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[0;32m-> 2070\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39masarray(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values, dtype\u001b[39m=\u001b[39mdtype)\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'Lil Baby'"
     ]
    }
   ],
   "source": [
    "#Logistic regression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logr = LogisticRegression()\n",
    "logr.fit(X_train,y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9e8995cc79525bd2f03bf06fe7351d86fcc477a350b98b661267de2f13aa6250"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
